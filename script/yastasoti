#!/usr/bin/env python

from os.path import realpath, dirname, join
import sys

sys.path.insert(0, join(dirname(realpath(sys.argv[0])), '..', 'src'))

from argparse import ArgumentParser
import json
import sys
import hashlib
import os
from time import sleep, localtime, strftime
import urllib

import requests
try:
    from tqdm import tqdm
except ImportError:
    def tqdm(x, **kwargs): return x


def url_to_dirname_and_filename(url):
    parts = url.split(u'/')
    parts = parts[2:]
    domain_name = parts[0]
    domain_name = urllib.quote_plus(domain_name)
    parts = parts[1:]
    filename = u'/'.join(parts)
    filename = urllib.quote_plus(filename.encode('utf-8'))
    if not filename:
        filename = 'index.html'
    return (domain_name, filename)


def compute_hash(filename):
    collector = hashlib.sha1()
    with open(filename, 'rb') as f:
        while True:
            data = f.read(1024)
            if not data:
                break
            collector.update(data)
    return collector.hexdigest()


def download(url, dirname, filename):
    response = requests.get(url, stream=True)
    partname = os.path.join(dirname, filename + '_part')
    with open(partname, "wb") as f:
        for data in response.iter_content():
            f.write(data)
    destname = os.path.join(dirname, filename)
    if os.path.exists(destname):
        desthash = compute_hash(destname)
        parthash = compute_hash(partname)
        if desthash == parthash:
            os.unlink(partname)
        else:
            mtime = os.path.getmtime(destname)
            timestring = strftime('%Y.%m%d.%H%M%S', localtime(mtime))
            archname = '{}_REV{}'.format(destname, timestring)
            os.rename(destname, archname)
            os.rename(partname, destname)
    else:
        os.rename(partname, destname)
    return response


delay_between_fetches = 0


def foreach_link(links, callback, article_root=None, missing_only=False, ignore_urls=[]):

    failures = []
    for link in tqdm(links, total=len(links)):
        url = link['url']
        if url in ignore_urls:
            continue
        try:
            if url.startswith(('#',)):
                continue
            elif not url.startswith(('http://', 'https://')):
                if '#' in url:
                    filename = url.split('#')[0]
                else:
                    filename = url
                filename = urllib.unquote(filename)
                filename = os.path.join(article_root, filename)
                if not os.path.exists(filename):
                    raise ValueError('Local file "{}" does not exist'.format(filename))
                continue
            else:
                response = callback(url)
            if response is None:
                continue
            status = response.status_code
        except Exception as e:
            status = str(e)
        if status not in (200, 301, 302, 303):
            failures.append({
                'status': status,
                'url': url,
                'link': link,
            })
        if delay_between_fetches > 0:
            sleep(delay_between_fetches)
    return failures


def check_links(links, **kwargs):
    def callback(url):
        return requests.head(url)
    return foreach_link(links, callback, **kwargs)


def archive_links_to(links, dest_dir, **kwargs):

    missing_only = kwargs.get('missing_only', False)

    def callback(url):
        dirname, filename = url_to_dirname_and_filename(url)
        dirname = os.path.join(dest_dir, dirname)
        if not os.path.exists(dirname):
            os.makedirs(dirname)
        if missing_only and os.path.exists(os.path.join(dirname, filename)):
            return None
        response = download(url, dirname, filename)
        return response

    return foreach_link(links, callback, **kwargs)


def main(args):
    argparser = ArgumentParser()

    # Checks if the links are resolvable, and optionally downloads a copy of each

    argparser.add_argument('input_files', nargs='+', metavar='FILENAME', type=str,
        help='JSON files containing the links to archive'
    )

    argparser.add_argument('--archive-links-to', metavar='DIRNAME', type=str, default=None,
        help='Download a copy of all web objects linked to from the entries'
    )
    argparser.add_argument('--archive-missing-only', action='store_true',
        help='When archiving links, only download the link if it is not already archived'
    )
    argparser.add_argument('--article-root', metavar='DIRNAME', type=str, default='.',
        help='Directory in which local files found when checking or archiving links should be located'
    )
    argparser.add_argument('--ignore-urls', metavar='URLS', type=str, default=None,
        help='Comma-separated list of link targets that should not even try to be fetched'
    )

    options = argparser.parse_args(sys.argv[1:])

    links = []
    for filename in options.input_files:
        if filename == '-':
            data = json.loads(sys.stdin.read())
        else:
            with open(filename, 'r') as f:
                data = json.loads(f.read())
        links.extend(data)

    if options.ignore_urls is None:
        options.ignore_urls = []
    else:
        options.ignore_urls = options.ignore_urls.split(',')

    if options.archive_links_to:
        result = archive_links_to(
            links,
            options.archive_links_to,
            article_root=options.article_root,
            missing_only=options.archive_missing_only,
            ignore_urls=options.ignore_urls,
        )
    else:
        result = check_links(
            links,
            article_root=options.article_root,
            missing_only=options.archive_missing_only,
            ignore_urls=options.ignore_urls,
        )

    sys.stdout.write(json.dumps(result, indent=4, sort_keys=True))


if __name__ == '__main__':
    main(sys.argv[1:])
