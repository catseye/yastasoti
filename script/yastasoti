#!/usr/bin/env python

from os.path import realpath, dirname, join
import sys

sys.path.insert(0, join(dirname(realpath(sys.argv[0])), '..', 'src'))

from argparse import ArgumentParser
import json
import sys
import hashlib
import os
from time import sleep, localtime, strftime
import urllib

import requests
try:
    from tqdm import tqdm
except ImportError:
    def tqdm(x, **kwargs): return x


def url_to_dirname_and_filename(url):
    parts = url.split(u'/')
    parts = parts[2:]
    domain_name = parts[0]
    domain_name = urllib.quote_plus(domain_name)
    parts = parts[1:]
    filename = u'/'.join(parts)
    filename = urllib.quote_plus(filename.encode('utf-8'))
    if not filename:
        filename = 'index.html'
    return (domain_name, filename)


def compute_hash(filename):
    collector = hashlib.sha1()
    with open(filename, 'rb') as f:
        while True:
            data = f.read(1024)
            if not data:
                break
            collector.update(data)
    return collector.hexdigest()


def download(url, dirname, filename):
    response = requests.get(url, stream=True)
    partname = os.path.join(dirname, filename + '_part')
    with open(partname, "wb") as f:
        for data in response.iter_content():
            f.write(data)
    destname = os.path.join(dirname, filename)
    if os.path.exists(destname):
        desthash = compute_hash(destname)
        parthash = compute_hash(partname)
        if desthash == parthash:
            os.unlink(partname)
        else:
            mtime = os.path.getmtime(destname)
            timestring = strftime('%Y.%m%d.%H%M%S', localtime(mtime))
            archname = '{}_REV{}'.format(destname, timestring)
            os.rename(destname, archname)
            os.rename(partname, destname)
    else:
        os.rename(partname, destname)
    return response


delay_between_fetches = 0

class LinkTraverser(object):
    def __init__(self, links, article_root=None, missing_only=False, ignore_urls=None):
        self.links = links
        self.article_root = article_root
        self.missing_only = missing_only
        self.ignore_urls = ignore_urls or []

    def handle_link(self, url):
        raise NotImplementedError

    def traverse(self):
        failures = []
        for link in tqdm(self.links, total=len(self.links)):
            url = link['url']
            if url in self.ignore_urls:
                continue
            try:
                if url.startswith(('#',)):
                    continue
                elif not url.startswith(('http://', 'https://')):
                    if '#' in url:
                        filename = url.split('#')[0]
                    else:
                        filename = url
                    filename = urllib.unquote(filename)
                    filename = os.path.join(article_root, filename)
                    if not os.path.exists(filename):
                        raise ValueError('Local file "{}" does not exist'.format(filename))
                    continue
                else:
                    response = self.handle_link(url)
                if response is None:
                    continue
                status = response.status_code
            except Exception as e:
                status = str(e)
            if status not in (200, 301, 302, 303):
                failures.append({
                    'status': status,
                    'url': url,
                    'link': link,
                })
            if delay_between_fetches > 0:
                sleep(delay_between_fetches)
        return failures


class LinkChecker(LinkTraverser):
    def handle_link(self, url):
        return requests.head(url)


class LinkArchiver(LinkTraverser):
    def __init__(self, links, dest_dir, **kwargs):
        super().__init__(links, **kwargs)
        self.dest_dir = dest_dir

    def handle_link(self, url):
        dirname, filename = url_to_dirname_and_filename(url)
        dirname = os.path.join(self.dest_dir, dirname)
        if not os.path.exists(dirname):
            os.makedirs(dirname)
        if self.missing_only and os.path.exists(os.path.join(dirname, filename)):
            return None
        response = download(url, dirname, filename)
        return response


def main(args):
    argparser = ArgumentParser()

    # Checks if the links are resolvable, and optionally downloads a copy of each

    argparser.add_argument('input_files', nargs='+', metavar='FILENAME', type=str,
        help='JSON files containing the links to archive'
    )

    argparser.add_argument('--archive-links-to', metavar='DIRNAME', type=str, default=None,
        help='Download a copy of all web objects linked to from the entries'
    )
    argparser.add_argument('--archive-missing-only', action='store_true',
        help='When archiving links, only download the link if it is not already archived'
    )
    argparser.add_argument('--article-root', metavar='DIRNAME', type=str, default='.',
        help='Directory in which local files found when checking or archiving links should be located'
    )
    argparser.add_argument('--ignore-urls', metavar='URLS', type=str, default=None,
        help='Comma-separated list of link targets that should not even try to be fetched'
    )

    options = argparser.parse_args(sys.argv[1:])

    links = []
    for filename in options.input_files:
        if filename == '-':
            data = json.loads(sys.stdin.read())
        else:
            with open(filename, 'r') as f:
                data = json.loads(f.read())
        links.extend(data)

    if options.ignore_urls is None:
        options.ignore_urls = []
    else:
        options.ignore_urls = options.ignore_urls.split(',')

    if options.archive_links_to:
        traverser = LinkArchiver(links, options.archive_links_to,
            article_root=options.article_root,
            missing_only=options.archive_missing_only,
            ignore_urls=options.ignore_urls,
        )
    else:
        traverser = LinkChecker(links,
            article_root=options.article_root,
            missing_only=options.archive_missing_only,
            ignore_urls=options.ignore_urls,
        )

    result = traverser.traverse()

    sys.stdout.write(json.dumps(result, indent=4, sort_keys=True))


if __name__ == '__main__':
    main(sys.argv[1:])
